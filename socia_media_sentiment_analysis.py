# -*- coding: utf-8 -*-
"""SOCIA_MEDIA_SENTIMENT_ANALYSIS.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1KYjdElX9SS4Pl1VW5KNrclyflzeI9e6J
"""

import re
import numpy as np
import pandas as pd

import nltk

# plotting
import seaborn as sns
from wordcloud import WordCloud
import matplotlib.pyplot as plt

# sklearn
from sklearn.naive_bayes import MultinomialNB
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.metrics import confusion_matrix, classification_report

FILE_PATH ='/content/drive/MyDrive/training.1600000.processed.noemoticon.csv'
COL_NAMES = ['target','ids','date','flag','user','text']
df = pd.read_csv(FILE_PATH, encoding = "ISO-8859-1", names = COL_NAMES)

df.head()

df.columns

df["target"].unique()

df.dtypes

df.shape

df.info()

df.isnull().sum()

df.duplicated().sum()

df = df[['text','target']]
df.info()

df['target'].unique()

df['target'] = df['target'].replace(to_replace = 4, value = 1)
df.tail()

sns.countplot(data= df, x = 'target')

df_pos = df[df['target'] == 1].iloc[:200000]
df_neg = df[df['target'] == 0].iloc[:200000]
df_new = pd.concat([df_neg,df_pos], axis = 0)

df_new['text'] = df_new['text'].str.lower()

sequencePattern   = r"(.)\1\1+"
seqReplacePattern = r"\1\1"

def removing_consecutive (data):
    return re.sub(sequencePattern, seqReplacePattern, data)

df_new['text'] = df_new['text'].apply(removing_consecutive)
df_new['text'].tail()

url_pattern = r"((http://)[^ ]*|(https://)[^ ]*|( www\.)[^ ]*)"

def cleaning_URLs(data):
    return re.sub(url_pattern,'',data)

df_new['text'] = df_new['text'].apply(cleaning_URLs)
df_new['text'].tail()

# Replacing emojis
# Define Emojis dictionary API
emojis = {':)': 'smile', ':-)': 'smile', ';d': 'wink', ':-E': 'vampire', ':(': 'sad',
          ':-(': 'sad', ':-<': 'sad', ':P': 'raspberry', ':O': 'surprised',
          ':-@': 'shocked', ':@': 'shocked',':-$': 'confused', ':\\': 'annoyed',
          ':#': 'mute', ':X': 'mute', ':^)': 'smile', ':-&': 'confused', '$_$': 'greedy',
          '@@': 'eyeroll', ':-!': 'confused', ':-D': 'smile', ':-0': 'yell', 'O.o': 'confused',
          '<(-_-)>': 'robot', 'd[-_-]b': 'dj', ":'-)": 'sadsmile', ';)': 'wink',
          ';-)': 'wink', 'O:-)': 'angel','O*-)': 'angel','(:-D': 'gossip', '=^.^=': 'cat'}

def replacing_emojis(data):
    text = []
    for word in data.split():
        if word not in emojis.keys():
            text.append(word)
        else:
            text.append(emojis[word])
    return ' '.join(text)

df_new['text'] = df_new['text'].apply(replacing_emojis)
df_new['text'].tail()

# Removing Tags
tag_pattern = '@[^\s]+'

def removing_tag (data):
    return re.sub(tag_pattern,'',data)

df_new['text'] = df_new['text'].apply(removing_tag)
df_new['text'].tail()

# Removing Punctuations
import string
english_punctuations = string.punctuation
punctuations_list = english_punctuations
def cleaning_punctuations(data):
    translator = str.maketrans('', '', punctuations_list)
    return data.translate(translator)

df_new['text']= df_new['text'].apply(cleaning_punctuations)
df_new['text'].tail()

# Cleaning stop words
# Define an English stopwords list (API)

stopwordlist = ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'youre', 'youve', 'youll', 'youd', 'your', 'yours',
                'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', 'shes', 'her', 'hers', 'herself', 'it', 'its', 'its', 'itself',
                'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', 'thatll', 'these', 'those', 'am',
                'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and',
                'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between',
                'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off',
                'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both',
                'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'only', 'own', 'same', 'so', 'than', 'too',
                'very', 's', 't', 'can', 'will', 'just', 'don', 'dont', 'should', 'shouldve', 'now', 'd', 'll', 'm', 'o', 're', 've', 'y',
                'ain', 'aren', 'arent', 'couldn', 'couldnt', 'didn', 'didnt', 'doesn', 'doesnt', 'hadn', 'hadnt', 'hasn', 'hasnt', 'haven',
                'havent', 'isn', 'isnt', 'ma', 'mightn', 'mightnt', 'mustn', 'mustnt', 'needn', 'neednt', 'shan', 'shant', 'shouldn', 'shouldnt',
                'wasn', 'wasnt', 'weren', 'werent', 'won', 'wont', 'wouldn', 'wouldnt']


# Stopwordlist was extracted from stopwords list function within nltk library and have been removed punctuations, while it do not have i'm, etc. we will add this.
stopwordlist += ["im", "theyre"]

def removing_stopword(data):
    text = []
    for word in data.split():
        if word not in stopwordlist:
            text.append(word)
    return ' '.join(text)

df_new['text'] = df_new['text'].apply(removing_stopword)
df_new['text'].tail()

#Removing single words
def removing_single(data):
    text = [word for word in data.split() if len(word)>1]
    return ' '.join(text)

df_new['text'] = df_new['text'].apply(removing_single)
df_new['text'].tail()

# First Lemmatization ( turn better -> good, etc.)
import nltk
from nltk.stem import WordNetLemmatizer
nltk.download('wordnet')
nltk.download('omw-1.4')
!unzip /usr/share/nltk_data/corpora/wordnet.zip -d /usr/share/nltk_data/corpora/


lm = WordNetLemmatizer()

def lemmatization_1(data):
    words = data.split()
    lemmatized_words = [lm.lemmatize(word) for word in words]
    return ' '.join(lemmatized_words)


df_new['text'] = df_new['text'].apply(lemmatization_1)
df_new['text'].tail()

# Second Lemmatization (V-ed, V2, Ving -> V)
from nltk import pos_tag
def lemmatization_2(data):
    words = data.split()
    lemmatized_words = [lm.lemmatize(word, pos = 'v') for word in words]
    return ' '.join (lemmatized_words)


df_new['text'] = df_new['text'].apply(lemmatization_2)
df_new['text'].tail()

data_neg = df_new[df_new['target']== 1].text
plt.figure(figsize = (20,20))
wc = WordCloud(max_words = 1000 , width = 1600 , height = 800,
               collocations=False).generate(" ".join(data_neg))
plt.imshow(wc)
plt.title ("Positive Words")

data_neg = df_new[df_new['target']== 0].text
plt.figure(figsize = (20,20))
wc = WordCloud(max_words = 1000 , width = 1600 , height = 800,
               collocations=False).generate(" ".join(data_neg))
plt.imshow(wc)
plt.title ("Negative Words")

X = df_new['text']
y = df_new['target']

from sklearn.model_selection import train_test_split
X_temp, X_test, y_temp, y_test = train_test_split (X,y, test_size = 0.2, random_state = 1211)
X_train, X_valid, y_train, y_valid = train_test_split (X_temp, y_temp, test_size = 0.2, random_state = 1211)

vectoriser = CountVectorizer(max_features=100000)
vectoriser.fit(X_train.values)
X_train_count = vectoriser.transform(X_train)
X_valid_count = vectoriser.transform(X_valid)
X_test_count = vectoriser.transform(X_test)

# Import Necessary Libraries
from sklearn.naive_bayes import MultinomialNB
from sklearn.linear_model import SGDClassifier
from sklearn.model_selection import GridSearchCV

clf_nb = MultinomialNB(alpha = 1.0, class_prior = None, fit_prior = True)
clf_nb.fit(X_train_count,y_train)

clf_nb.score (X_train_count, y_train)

clf_nb.score (X_valid_count, y_valid)

from sklearn.linear_model import SGDClassifier
from sklearn.model_selection import GridSearchCV

model_sgd = SGDClassifier(max_iter=1000, n_jobs=-1, loss = 'log_loss')

param_grid_sgd = {
    'alpha': [0.0001, 0.001, 0.01],
    'penalty': ['l2', 'l1'] # Adding regularization to minimize overfitting
}

clf_sgd = GridSearchCV(model_sgd, param_grid_sgd, cv=3, return_train_score=False)

clf_sgd.fit(X_train_count, y_train)

best_params_sgd = clf_sgd.best_params_
best_score_sgd = clf_sgd.best_score_
clf_sgd_best = clf_sgd.best_estimator_

print("Best Hyperparameters:", best_params_sgd)
print("Best Score:", best_score_sgd)

clf_sgd_best.score(X_train_count, y_train)

clf_sgd_best.score(X_valid_count, y_valid)

from sklearn.metrics import roc_curve, roc_auc_score

y_nb_probs = clf_nb.predict_proba(X_valid_count)[:,1]
y_lr_probs = clf_sgd_best.predict_proba(X_valid_count)[:,1]

auc_nb = roc_auc_score(y_valid, y_nb_probs)
auc_lr = roc_auc_score(y_valid, y_lr_probs)

lw = 2

fpr_nb, tpr_nb,thresholds_nb = roc_curve(y_valid, y_nb_probs, pos_label=1)
fpr_lr, tpr_lr,thresholds_lr = roc_curve(y_valid, y_lr_probs, pos_label=1)

plt.plot(fpr_nb, tpr_nb, lw = 2, color ='green', label=f'Naive Bayes ROC curve (area = {auc_nb:.2f})' )
plt.plot(fpr_lr, tpr_lr, lw = 2, color ='red', label=f'Logistic Regression ROC curve (area = {auc_lr:.2f})' )

plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic')
plt.legend(loc="lower right")
plt.show()

clf_sgd_best.score(X_test_count, y_test)

y_pred = clf_sgd_best.predict(X_test_count)

from sklearn.metrics import classification_report
print (classification_report( y_pred, y_test))

from sklearn.metrics import confusion_matrix
from sklearn.metrics import ConfusionMatrixDisplay

mat = confusion_matrix (y_pred, y_test)
dis = ConfusionMatrixDisplay( confusion_matrix = mat)
dis.plot()
plt.tight_layout()
plt.show()

from sklearn.metrics import roc_curve, roc_auc_score

y_lr_probs = clf_sgd_best.predict_proba(X_test_count)[:,1]
auc_lr = roc_auc_score(y_test, y_lr_probs)

lw = 2
fpr_lr, tpr_lr,thresholds_lr = roc_curve(y_test, y_lr_probs, pos_label=1)
plt.plot(fpr_lr, tpr_lr, lw = 2, color ='red', label=f'Logistic Regression ROC curve (area = {auc_lr:.2f})' )

plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic')
plt.legend(loc="lower right")
plt.show()

import re
def predict_sentiment(text):
  # Clean the text
  # Define sequencePattern and seqReplacePattern here
  sequencePattern = r"(.)\1\1+"
  seqReplacePattern = r"\1\1"
  text = text.lower()
  text = re.sub(sequencePattern, seqReplacePattern, text) # Replace 3 or more consecutive characters with 2 characters
  text = cleaning_URLs(text)
  text = replacing_emojis(text)
  text = removing_tag(text)
  text = cleaning_punctuations(text)
  text = removing_stopword(text)
  text = removing_single(text)
  text = lemmatization_1(text)
  text = lemmatization_2(text)

  # Vectorize the text
  text_count = vectoriser.transform([text])

  # Predict the sentiment
  prediction = clf_nb.predict(text_count)[0]

  # Return the prediction
  if prediction == 1:
    return "Positive"
  else:
    return "Negative"

# Get the input text from the user
input_text = input("Enter your text: ")

# Predict the sentiment
sentiment = predict_sentiment(input_text)

# Print the sentiment
print(f"The sentiment of the text is: {sentiment}")